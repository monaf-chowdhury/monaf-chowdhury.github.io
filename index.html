<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Abdul Monaf Chowdhury </title> <meta name="author" content="Abdul Monaf Chowdhury"> <meta name="description" content="A student of University of Dhaka, focusing on Multi-modal Learning "> <meta name="keywords" content="CV, NLP, RL"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monaf-chowdhury.github.io/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6D%6F%6E%61%66%61%62%64%75%6C%31%35@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/monaf-chowdhury" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.kaggle.com/monafchowdhury" title="Kaggle" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-kaggle"></i></a> <a href="https://www.linkedin.com/in/monaf-chowdhury" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://scholar.google.com/citations?user=LBoyI9cAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/monaf_chowdhury" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/resume.pdf">resume </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Abdul Monaf</span> Chowdhury </h1> <p class="desc">Research Assistant @ <a href="https://www.maimlab.com/" rel="external nofollow noopener" target="_blank">University of Dhaka, Bangladesh</a> <br> CS PhD Aspirant</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic.png?95f7999dfce5716f38fca12af7956458" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>I am a Research Assistant at the <a href="https://www.maimlab.com/" rel="external nofollow noopener" target="_blank">MAIM Lab</a> of Robotics &amp; Mechatronics Engineering in the University of Dhaka, Bangladesh. Currently, I am working on a <a href="https://wellcomeleap.org/inutero/" rel="external nofollow noopener" target="_blank">Wellcome Leap - In Utero</a> funded project (Title: Translation of a Wearable Fetal Movement Monitor towards Stillbirth Prevention), collaborating with University College Dublin to develop a wearable device to monitor fetal movements to cut stillbirth rates.</p> <p>Recently, I graduated from the University of Dhaka in Robotics and Mechatronics Engineering, supervised by <a href="https://www.du.ac.bd/faculty/faculty_details/HSS/4706" rel="external nofollow noopener" target="_blank">Dr. Md Mehedi Hasan</a>. My fourth year thesis project was on “Enhancing UAV Based Human Action Recognition: A Deep Learning Approach”. <a href="https://drive.google.com/file/d/1im1cmDKfGHdqRqx8z4r-s_zfVFTyEj2g/view?usp=sharing" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">[Report]</code></a></p> <hr> <h2 id="research-interest"><strong>Research Interest</strong></h2> <p>I’m interested in <code class="language-plaintext highlighter-rouge">Multi-modal Learning</code>, especially at the fusion of Vision + Language. Besides, I’m also interested in Computer Vision, Deep Learning, and Machine Learning, especially reinforcement learning. Overall, I would like to work on fusing complementary intelligence from multi-modalities with the ambitious goal of positively influencing real-world environments via embodied agents.</p> <p>I have extensively collaborated on interdisciplinary projects in areas ranging from Machine Learning and Deep learning to Health Informatics and Biomechatronics. My current work revolves around machine learning challenges related to biomedical sensor data, e.g., learning with limited skewed data, supervised learning, and transfer learning with applications to Medical AI and Biomechatronics.</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jul 31, 2025</th> <td> Submitted my work on Long Term Time Series Forecasting to <code class="language-plaintext highlighter-rouge">AAAI '26</code>, and the paper is published at <a href="https://www.arxiv.org/abs/2508.04251" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">ArXiv</code></a> </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 08, 2024</th> <td> My journal paper titled <a href="https://drive.google.com/file/d/11J1URbFJxXePijMYvL8VcGCqjXLpQGZx/view" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">U-ActionNet: Dual-Pathway Fourier Networks with Region-of-Interest Module for Efficient Action Recognition in UAV Surveillance</code></a>, based on my fourth year thesis work, has been published in <em>IEEE Access</em> </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 31, 2024</th> <td> Our work on stillbirth prevention got another year of funding extension worth <strong>$1M</strong> by <a href="https://wellcomeleap.org/inutero/" rel="external nofollow noopener" target="_blank">Wellcome Leap - In Utero</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 01, 2024</th> <td> Started working as a research assistant at the <a href="https://www.maimlab.com/" rel="external nofollow noopener" target="_blank">MAIM Lab, University of Dhaka</a> under the supervision of <a href="https://people.ucd.ie/niamh.nowlan" rel="external nofollow noopener" target="_blank">Professor Niamh Nowlan</a> and <a href="https://www.du.ac.bd/faculty/faculty_details/RME/2318" rel="external nofollow noopener" target="_blank">Dr. Abhishek Kumar Ghosh</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 23, 2024</th> <td> Graduated from <a href="https://www.du.ac.bd/" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">University of Dhaka, Bangladesh</code></a> with a BSc. in Robotics &amp; Mechatronics Engineering. <a href="https://drive.google.com/file/d/1im1cmDKfGHdqRqx8z4r-s_zfVFTyEj2g/view" rel="external nofollow noopener" target="_blank">[Report]</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 09, 2023</th> <td> Attended and presented my <code class="language-plaintext highlighter-rouge">FFTUAVNet</code> paper to the <em>5th IEEE International Conference on Sustainable Technologies for Industry 5.0 (STI)</em> conference. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 06, 2023</th> <td> My paper titled <a href="https://drive.google.com/file/d/1V5aXfI4T6AOKbXE_85453PILYPtT9Y5e/view" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">FFT-UAVNet: FFT Based Human Action Recognition for Drone Surveillance System</code></a> got accepted to the <em>5th IEEE International Conference on Sustainable Technologies for Industry 5.0 (STI)</em> conference </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <p>See my full publications at <a href="https://monaf-chowdhury.github.io/publications/">the publication page</a>!</p> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ArXiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/t3_time.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="t3_time.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chowdhury2025t3time" class="col-sm-8"> <div class="title">T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head Alignment and Residual Fusion</div> <div class="author"> <em>Abdul Monaf Chowdhury</em>, Rabeya Akter, and Safaeid Hossain Arib </div> <div class="periodical"> <em>arXiv preprint arXiv:2508.04251</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.arxiv.org/abs/2508.04251" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/monaf-chowdhury/T3Time" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Multivariate time series forecasting (MTSF) seeks to model temporal dynamics among variables to predict future trends. Transformer-based models and large language models (LLMs) have shown promise due to their ability to capture long-range dependencies and patterns. However, current methods often rely on rigid inductive biases, ignore intervariable interactions, or apply static fusion strategies that limit adaptability across forecast horizons. These limitations create bottlenecks in capturing nuanced, horizon-specific relationships in time-series data. To solve this problem, we propose T3Time, a novel trimodal framework consisting of time, spectral, and prompt branches, where the dedicated frequency encoding branch captures the periodic structures along with a gating mechanism that learns prioritization between temporal and spectral features based on the prediction horizon. We also proposed a mechanism which adaptively aggregates multiple cross-modal alignment heads by dynamically weighting the importance of each head based on the features. Extensive experiments on benchmark datasets demonstrate that our model consistently outperforms state-of-the-art baselines, achieving an average reduction of 3.28% in MSE and 2.29% in MAE. Furthermore, it shows strong generalization in few-shot learning settings: with 5% training data, we see a reduction in MSE and MAE by 4.13% and 1.91%, respectively; and with 10% data, by 3.62% and 1.98% on average.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chowdhury2025t3time</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head Alignment and Residual Fusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chowdhury, Abdul Monaf and Akter, Rabeya and Arib, Safaeid Hossain}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2508.04251}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.arxiv.org/abs/2508.04251}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Access</abbr> <figure> <picture> <img src="/assets/img/publication_preview/u_action_model.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="u_action_model.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chowdhury2024u" class="col-sm-8"> <div class="title">U-ActionNet: Dual-pathway fourier networks with region-of-interest module for efficient action recognition in UAV surveillance</div> <div class="author"> <em>Abdul Monaf Chowdhury</em>, Ahsan Imran, Md Mehedi Hasan, Riad Ahmed, Akm Azad, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Salem A Alyami' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>IEEE Access</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://drive.google.com/file/d/11J1URbFJxXePijMYvL8VcGCqjXLpQGZx/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Unmanned Aerial Vehicles (UAV) have revolutionized human action recognition by offering a bird’s-eye perspective, thereby unlocking unprecedented potential for comprehensive support within surveillance systems. This paper presents synergistic strategies for enhancing Human Action Recognition (HAR) in UAV imagery. Leveraging a dual-path approach, we propose a novel framework U-ActionNet that integrates FFT-based substance separation and space-time self-attention techniques to improve the accuracy and efficiency of HAR tasks. Catering the server side the first pathway employs a modified C3D model with Fast Fourier Transform (FFT)-based object movement and attention detection mechanisms to effectively extract human actors from complex scenes and capture spatiotemporal dynamics in UAV footage. Moreover, a generalized Region-of-Interest (ROI) module is utilized to concentrate on optimal regions to enhance target recognition. Through extensive experiments on the Drone Action and UAV-human datasets, we demonstrate the effectiveness of our approach, achieving superior performance compared to the state-of-the-art methods with Top-1 accuracies of 94.94% and 95.05%, respectively. Meanwhile, the second pathway employs edge-optimized models, integrating ROI extraction-based frame sampling technique to eliminate static frames while preserving pivotal frames essential for model training. We introduce a lightweight model named U-ActionNet Light which is a combination of MobileNetV2, Fourier module, and BiLSTM models that demand only one-ninth of the parameters compared to the server-side model. Demonstrating its efficacy, this model attains Top-1 accuracies of 80.43% and 84.74% on the same datasets, surpassing the baseline by a significant margin. Experimental results show that the presented frameworks are promising for surveillance, search and rescue, and activity monitoring, where accurate and real-time human action recognition from UAV platforms is essential.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chowdhury2024u</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{U-ActionNet: Dual-pathway fourier networks with region-of-interest module for efficient action recognition in UAV surveillance}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chowdhury, Abdul Monaf and Imran, Ahsan and Hasan, Md Mehedi and Ahmed, Riad and Azad, Akm and Alyami, Salem A}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Access}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">STI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/u_action_abs.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="u_action_abs.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chowdhury2023fft" class="col-sm-8"> <div class="title">FFT-UAVNet: FFT Based Human Action Recognition for Drone Surveillance System</div> <div class="author"> <em>Abdul Monaf Chowdhury</em>, Ahsan Imran, and Md Mehedi Hasan </div> <div class="periodical"> <em>In 2023 5th International Conference on Sustainable Technologies for Industry 5.0 (STI)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://drive.google.com/file/d/1V5aXfI4T6AOKbXE_85453PILYPtT9Y5e/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Unmanned aerial vehicles (UAVs) have emerged as a transformative technology for human action recognition, providing a birds-eye view and unlocking new possibilities for precise and comprehensive support in surveillance systems. While substantial advances in ground-based human action recognition have been achieved, the unique characteristics of UAV footage present new challenges that require tailored solutions. Specifically, the reduced scale of humans in aerial perspectives necessitates the development of specialised models to accurately recognize and interpret human actions. Our research focuses on modifying the well-established C3D model and incorporating Fast Fourier Transform (FFT)-based object disentanglement (FO) and space-time attention (FA) mechanisms. By leveraging the power of FFT, our model effectively disentangles the human actors from the background and captures the spatio-temporal dynamics of human actions in UAV footage, enhancing the discriminative capabilities and enabling accurate action recognition. Through extensive experimentation on a subset of the UAV-Human dataset, our proposed FFT-UAVNet (m-C3D+FO&amp;FA+FC) model demonstrates remarkable improvements in performance. We achieve a Top-1 accuracy of 64.86% and a Top-3 accuracy of 83.37%, surpassing the results obtained by the standard C3D and X3D methods, which achieve only a Top-1 accuracy of 28.05% and 31.33%, respectively. These findings underscore the efficacy of our approach and emphasize the significance of the proposed model for UAV datasets in maximizing the potential of UAV-based human action recognition.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chowdhury2023fft</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FFT-UAVNet: FFT Based Human Action Recognition for Drone Surveillance System}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chowdhury, Abdul Monaf and Imran, Ahsan and Hasan, Md Mehedi}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 5th International Conference on Sustainable Technologies for Industry 5.0 (STI)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--6}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6D%6F%6E%61%66%61%62%64%75%6C%31%35@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/monaf-chowdhury" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.kaggle.com/monafchowdhury" title="Kaggle" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-kaggle"></i></a> <a href="https://www.linkedin.com/in/monaf-chowdhury" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://scholar.google.com/citations?user=LBoyI9cAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/monaf_chowdhury" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note">You can even add a little note about which of these is the best way to reach you. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Abdul Monaf Chowdhury. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>